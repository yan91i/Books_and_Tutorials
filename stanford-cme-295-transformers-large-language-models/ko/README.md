# 트랜스포머와 대형 언어 모델, 스탠포드 대학교, CME 295 핵심 요약본
지원 언어: [العربية](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ar) - [English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) - [Español](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/es) - [فارسی](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fa) - [Français](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fr) - [Italiano](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/it) - [日本語](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ja) - **한국어** - [ไทย](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/th) - [Türkçe](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/tr) - [中文](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/zh)

## 목적
이 깃허브 리포지토리는 Stanford의 CME 295 트랜스포머와 대형 언어 모델 강의에서 배우는 중요한 개념들을 한 곳에 모아서 간편하게 살펴볼 수 있도록 정리한 것입니다. 포함 내용은 다음과 같습니다:
- **트랜스포머 (Transformers)**: 셀프 어텐션 (self-attention), 아키텍처, 변형, 최적화 기법 (스파스 어텐션, 로랭크 어텐션, 플래시 어텐션)
- **대형 언어 모델 (LLMs)**: 프롬프팅 (prompting), 파인 튜닝 (SFT, LoRA), 선호도 튜닝, 최적화 기법 (mixture of experts), 지식 증류 (distillation), 양자화 (quantization)
- **응용**: LLM-as-a-Judge (LLM 으로 다른 모델을 평가하는 방식), RAG, 에이전트, 추론 모델, DeepSeek-R1을 활용한 학습 단계 (train-time) 및 테스트 단계 (test-time) 에서의 스케일링

## 내용
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/ko/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-ko.png" alt="Illustration" width="600px"/></a>

## 수업 교재
이 VIP Cheatsheet 은 250페이지에 걸쳐 약 600장의 일러스트로 구성된 「Super Study Guide: Transformers & Large Language Models」 책에 담긴 내용을 간단히 요약해 보여줍니다. 더 자세한 내용은 https://superstudy.guide 에서 확인하실 수 있습니다.

## 수업 웹사이트
[cme295.stanford.edu](https://cme295.stanford.edu/)

## 저자
[압신 아미디](https://www.linkedin.com/in/afshineamidi/) (에콜 상트랄 파리, MIT) and [셰르빈 아미디](https://www.linkedin.com/in/shervineamidi/) (에콜 상트랄 파리, 스탠포드 대학교)

## 옮긴이
YJ (Yongjin) Kim 김용진

# 斯坦福大学 CME 295 课程：Transformer 与大语言模型速查表 
可用语言：[العربية](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ar) - [English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) - [Español](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/es) - [فارسی](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fa) - [Français](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fr) - [Italiano](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/it) - [日本語](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ja) - [한국어](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ko) - [ไทย](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/th) - [Türkçe](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/tr) - **中文**

## 目标
本仓库旨在集中汇总斯坦福大学 CME 295 “Transformer 与大语言模型”课程所涵盖的所有核心概念。其内容包括：
- **Transformer**：自注意力机制、架构、变体、优化技术（如稀疏注意力、低秩注意力、Flash Attention）
- **大语言模型 (LLM)**：提示 (prompting)、微调（SFT、LoRA）、偏好调优、优化技术（混合专家模型、知识蒸馏、量化）
- **应用**：LLM 作为评判者、检索增强生成 (RAG)、智能体、推理模型（来自 DeepSeek-R1 的训练时与测试时缩放技术）

## 内容
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/zh/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-zh.png" alt="Illustration" width="600px"/></a>

## 课程教材
这份 VIP 速查表概述了《Super Study Guide: Transformer 与大语言模型》一书的核心内容。该书包含约 600 幅插图，全书超过 250 页。更多详情请访问：https://superstudy.guide

## 课程网站
[cme295.stanford.edu](https://cme295.stanford.edu/)

## 作者
[Afshine Amidi](https://www.linkedin.com/in/afshineamidi/) (巴黎中央理工学院, 麻省理工学院) 和 [Shervine Amidi](https://www.linkedin.com/in/shervineamidi/) (巴黎中央理工学院, 斯坦福大学)

## 译者
[Tao Yu（俞涛）](https://www.linkedin.com/in/taoyucmu/) 和 [Binbin Xiong（熊斌斌）](https://www.linkedin.com/in/binbin-xiong-51ab8a43/)

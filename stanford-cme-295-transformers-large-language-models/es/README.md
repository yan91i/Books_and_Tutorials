# Guía del curso sobre Transformers y Grandes Modelos de Lenguaje para el curso CME 295 de Stanford
Disponible en: [العربية](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ar) - [English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) - **Español** - [فارسی](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fa) - [Français](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fr) - [Italiano](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/it) - [日本語](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ja) - [한국어](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ko) - [ไทย](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/th) - [Türkçe](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/tr) - [中文](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/zh)

## Objetivo
Este repositorio tiene como objetivo resumir en un solo lugar todas las nociones importantes que se cubren en el curso de CME 295 Transformers & Large Language Models de Stanford. Incluye:
- **Transformers**: auto-atención, arquitectura, variantes, técnicas de optimización (atención dispersa, atención de bajo rango, atención flash)
- **LLMs**: prompting, ajuste fino (SFT, LoRA), ajuste de preferencias, técnicas de optimización (mezcla de expertos, destilación, cuantización)
- **Aplicaciones**: LLM como juez, RAG, agentes, modelos de razonamiento (escalado en tiempo de entrenamiento y en tiempo de prueba de DeepSeek-R1)

## Contenido
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/es/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-es.png" alt="Illustration" width="600px"/></a>

## Libro de texto
Esta VIP cheatsheet ofrece un resumen del contenido del libro "Super Study Guide: Transformers & Large Language Models”, que incluye ~600 ilustraciones a lo largo de 250 páginas. Puedes encontrar más detalles en https://superstudy.guide.

## Sitio web de la clase
[cme295.stanford.edu](https://cme295.stanford.edu/)

## Autores
[Afshine Amidi](https://www.linkedin.com/in/afshineamidi/) (Ecole Centrale Paris, MIT) y [Shervine Amidi](https://www.linkedin.com/in/shervineamidi/) (Ecole Centrale Paris, Stanford University)

## Traductores
Steven Van Vaerenbergh (Universidad de Cantabria) y Lara Lloret Iglesias (Instituto de Física de Cantabria - CSIC/UC)

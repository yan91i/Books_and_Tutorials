# Pense-bête de Transformeurs et LLMs pour CME 295 de Stanford
Disponible en [العربية](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ar) - [English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) - [Español](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/es) - [فارسی](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fa) - **Français** - [Italiano](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/it) - [日本語](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ja) - [한국어](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ko) - [ไทย](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/th) - [Türkçe](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/tr) - [中文](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/zh)

## But
Ce repo a pour but de résumer toutes les notions importantes du cours de Transformeurs et LLMs (CME 295) de Stanford. En particulier, il inclut :
- **Transformeurs**: auto-attention, architecture, variants, optimisations (attention éparse, attention de bas rang, flash attention)
- **LLMs**: prompting, ajustement fin (SFT, LoRA), ajustement par préférence, optimisations (mélange d'experts, distillation, quantification)
- **Applications**: LLM comme juge, RAG, agents, modèles de raisonnement (extension à l'inférence et à l'entraînement de DeepSeek-R1)

## Contenu
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/fr/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-fr.png" alt="Illustration" width="600px"/></a>

## Manuel scolaire
Cette VIP cheatsheet donne un aperçu du contenu du livre intitulé « Super Study Guide : Transformeurs et Grands Modèles de Langage », qui contient environ 600 illustrations réparties sur 250 pages. Pour plus d'information, veuillez visiter : https://superstudy.guide.

## Site du cours
[cme295.stanford.edu](https://cme295.stanford.edu/)

## Auteurs
[Afshine Amidi](https://www.linkedin.com/in/afshineamidi/) (École Centrale Paris, MIT) et [Shervine Amidi](https://www.linkedin.com/in/shervineamidi/) (École Centrale Paris, Université de Stanford)

# สรุปเนื้อหาเรื่องตัวแปลงและแบบจำลองภาษาขนาดใหญ่ สำหรับรายวิชา CME 295 ม.สแตนฟอร์ด
ฉบับภาษาต่างๆ: [العربية](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ar) - [English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) - [Español](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/es) - [فارسی](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fa) - [Français](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fr) - [Italiano](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/it) - [日本語](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ja) - [한국어](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ko) - **ไทย** - [Türkçe](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/tr) - [中文](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/zh)

## เป้าหมาย
คลังนี้มุ่งรวบรวมหัวข้อสำคัญทั้งหมดที่กล่าวถึงในรายวิชา “CME 295 ตัวแปลงและแบบจำลองภาษาขนาดใหญ่” ของมหาวิทยาลัยสแตนฟอร์ดมาไว้ในที่เดียว มีเนื้อหาดังนี้:
- **ตัวแปลง (transformer)**: การเพ่งตน (self-attention), สถาปัตยกรรม (architecture), รูปแปรต่างๆ (variants), กลวิธีเพิ่มประสิทธิภาพ (เพ่งโหรง sparse attention, เพ่งแรงก์ต่ำ low-rank attention, เพ่งแฟลช flash attention)
- **LLM (แบบจำลองภาษาขนาดใหญ่)**: การบอก (prompting), การปรับละเอียด (finetuning; โดยใช้ SFT, LoRA), การปรับความชอบ (preference tuning), กลวิธีเพิ่มประสิทธิภาพ (เหล่าผู้เชี่ยวชาญ mixture of experts, การกลั่น distillation, การแจงหน่วย quantization)
- **การประยุกต์ใช้**: กรรมการ LLM (LLM-as-a-judge), RAG, ตัวแทน (agents), แบบจำลองให้เหตุผล (reasoning models), การย่อขยายตอนฝึกและตอนใช้งาน (train-time and test-time scaling) จาก DeepSeek-R1

## เนื้อหา
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/th/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-th.png" alt="Illustration" width="600px"/></a>

## หนังสือเรียนประกอบรายวิชา
VIP Cheatsheet นี้ ให้ภาพรวมของเนื้อหาในหนังสือ "Super Study Guide: Transformers & Large Language Models" ซึ่งบรรจุภาพประกอบกว่า 600 ภาพใน 250 หน้า ผู้สนใจสามารถดูรายละเอียดเพิ่มเติมได้ที่ https://superstudy.guide

## เว็บไซต์รายวิชา
[cme295.stanford.edu](https://cme295.stanford.edu/)

## ผู้เขียน
[อัฟชิน อามีดี](https://www.linkedin.com/in/afshineamidi/) (Ecole Centrale Paris, MIT) และ [เชอร์วิน อามิดี](https://www.linkedin.com/in/shervineamidi/) (Ecole Centrale Paris, Stanford University)

## ผู้แปล
บดินทร์ พรวิลาวัณย์ (นักแปลอิสระ)
ชารินทร์ พลภาณุมาศ (นักแปลอิสระ)

# Trasformatori e Modelli di linguaggio di grandi dimensioni, cheasheets per Stanford's CME 295
Disponibile in: [العربية](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ar) - [English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) - [Español](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/es) - [فارسی](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fa) - [Français](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fr) - **Italiano** - [日本語](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ja) - [한국어](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/ko) - [ไทย](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/th) - [Türkçe](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/tr) - [中文](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/zh)

## Obiettivo
Questo repository ha lo scopo di riassumere nello stesso luogo tutte le nozioni importanti che vengono trattate nel corso CME 295 Transformers & Large Language Models di Stanford. Include:
- **Trasformatori**: auto-attenzione, architettura, varianti, tecniche di ottimizzazione (sparse attention, low-rank attention, flash attention)
- **LLMs**: prompting, finetuning (SFT, LoRA), preference tuning, tecniche di ottimizzazione (mixture of experts, distillation, quantization)
- **Applicazioni**: LLM come giudice, RAG, agenti, modelli di ragionamento (scaling train-time e test-time da DeepSeek-R1)

## Contenuto
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/it/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-it.png" alt="Illustration" width="600px"/></a>

## Libro di testo del corso
Questo cheatsheet VIP fornisce una panoramica del contenuto del libro “Super Study Guide: Transformers & Large Language Models”, che contiene circa 600 illustrazioni in 250 pagine. Ulteriori dettagli sono disponibili sul sito https://superstudy.guide.

## Sito web del corso
[cme295.stanford.edu](https://cme295.stanford.edu/)

## Autori
[Afshine Amidi](https://www.linkedin.com/in/afshineamidi/) (Ecole Centrale Paris, MIT) e [Shervine Amidi](https://www.linkedin.com/in/shervineamidi/) (Ecole Centrale Paris, Università di Stanford)

## Traduttore
[Gianluca Guzzetta](https://www.linkedin.com/in/gianluca-guzzetta-1a778015b/) (PoliTO, UniSalento) 
